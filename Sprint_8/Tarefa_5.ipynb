{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente iremos preparar o ambiente, definindo o diretório onde nosso código será desenvolvido. Para este diretório iremos copiar o arquivo nomes_aleatorios.txt.\n",
    "\n",
    "Após, em nosso script Python, devemos importar as bibliotecas necessárias:\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "```\n",
    "Aplicando as bibliotecas do Spark, podemos definir a Spark Session e sobre ela definir o Context para habilitar o módulo SQL\n",
    "\n",
    "```spark = SparkSession \\\n",
    "\n",
    "                .builder \\\n",
    "\n",
    "                .master(\"local[*]\")\\\n",
    "\n",
    "                .appName(\"Exercicio Intro\") \\\n",
    "\n",
    "                .getOrCreate()```\n",
    "\n",
    "Nesta etapa, adicione código para ler o arquivo nomes_aleatorios.txt através do comando spark.read.csv. Carregue-o para dentro de um dataframe chamado df_nomes e, por fim, liste algumas linhas através do método show. Exemplo: df_nomes.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/11 19:35:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "+----------------+\n",
      "|             _c0|\n",
      "+----------------+\n",
      "|    David Staten|\n",
      "|  Heather Rivera|\n",
      "|     Lester Wood|\n",
      "|William Sandoval|\n",
      "| Gwendolyn Hines|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa a biblioteca findspark para iniciar o serviço do Spark na máquina que hospeda o meu notebook\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "# Importa demais bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "# Aplica as bibliotefcas do spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"Exercicio Intro\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lê o arquivo de texto\n",
    "df_nomes = spark.read.csv(\"Files/nomes_aleatorios.txt\")\n",
    "\n",
    "# Exibe 5 linhas do dataframe\n",
    "df_nomes.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Python, é possível acessar uma coluna de um objeto dataframe pelo atributo (por exemplo df_nomes.nome) ou por índice (df_nomes['nome']). Enquanto a primeira forma é conveniente para a exploração de dados interativos, você deve usar o formato de índice, pois caso algum nome de coluna não esteja de acordo seu código irá falhar.\n",
    "\n",
    "Como não informamos no momento da leitura do arquivo, o Spark não identificou o Schema por padrão e definiu todas as colunas como string. Para ver o Schema, use o método df_nomes.printSchema().\n",
    "\n",
    "Nesta etapa, será necessário adicionar código para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nomes: string (nullable = true)\n",
      "\n",
      "+------------------+\n",
      "|             Nomes|\n",
      "+------------------+\n",
      "|      David Staten|\n",
      "|    Heather Rivera|\n",
      "|       Lester Wood|\n",
      "|  William Sandoval|\n",
      "|   Gwendolyn Hines|\n",
      "|Constance Mcmillan|\n",
      "|        Billy Diaz|\n",
      "|      Marcos Rolfe|\n",
      "|         Cara Cole|\n",
      "|         John Gant|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeia a coluna para Nomes\n",
    "df_nomes = df_nomes.withColumnRenamed(\"_c0\", \"Nomes\")\n",
    "# Exibe o esquema\n",
    "df_nomes.printSchema()\n",
    "# Exibe 10 linhas do dataframe\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada Escolaridade e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Medio ou Superior.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/column.py:419: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|             Nomes|Escolaridade|\n",
      "+------------------+------------+\n",
      "|      David Staten|    Superior|\n",
      "|    Heather Rivera| Fundamental|\n",
      "|       Lester Wood|       Medio|\n",
      "|  William Sandoval| Fundamental|\n",
      "|   Gwendolyn Hines|       Medio|\n",
      "|Constance Mcmillan|       Medio|\n",
      "|        Billy Diaz| Fundamental|\n",
      "|      Marcos Rolfe| Fundamental|\n",
      "|         Cara Cole|    Superior|\n",
      "|         John Gant| Fundamental|\n",
      "+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa a biblioteca functions\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_nomes = df_nomes.withColumn(\n",
    "  # Gera uma coluna com nome Escolaridade com um array contendo os valores declarados\n",
    "  'Escolaridade',\n",
    "  f.array(\n",
    "  f.lit('Fundamental'),\n",
    "  f.lit('Medio'),\n",
    "  f.lit('Superior'),\n",
    "  # Seleciona de forma aleatória os valores declarados para a coluna Escolaridade\n",
    "  ).getItem((f.rand()*3).cast(\"int\")\n",
    "  )\n",
    ")\n",
    "# Exibe 10 linhas do dataframe\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada Pais e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+---------+\n",
      "|             Nomes|Escolaridade|     Pais|\n",
      "+------------------+------------+---------+\n",
      "|      David Staten|    Superior| Paraguai|\n",
      "|    Heather Rivera| Fundamental|Argentina|\n",
      "|       Lester Wood|       Medio|  Bolívia|\n",
      "|  William Sandoval| Fundamental| Suriname|\n",
      "|   Gwendolyn Hines|       Medio|  Bolívia|\n",
      "|Constance Mcmillan|       Medio|   Brasil|\n",
      "|        Billy Diaz| Fundamental|  Equador|\n",
      "|      Marcos Rolfe| Fundamental|  Bolívia|\n",
      "|         Cara Cole|    Superior|Venezuela|\n",
      "|         John Gant| Fundamental|  Equador|\n",
      "+------------------+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importa as bibliotecas e funções necessárias\n",
    "import hashlib\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "# Gera um array contendo o nome dos paíes da América do Sul\n",
    "countries = ['Argentina', 'Bolívia', 'Brasil', 'Chile', 'Colômbia', 'Equador', 'Guiana', 'Guiana Francesa', 'Paraguai', 'Peru', 'Suriname', 'Uruguai', 'Venezuela']\n",
    "\n",
    "# Define a função que seleciona aleatoriamente países da lista anterior\n",
    "def choose_country(x):\n",
    "  # Usa a função SHA-256 hash para gerar um valor de hash de 32-byte\n",
    "  hash_value = hashlib.sha256(str(x).encode()).digest()\n",
    "  # Converte os primeiros 2 bytes do valor do hash em inteiro\n",
    "  random_int = int.from_bytes(hash_value[:2], byteorder=\"big\")\n",
    "  # Calcula o índice do nome aleatório do país aleatório\n",
    "  index = random_int % len(countries)\n",
    "  # Retorna o nome do país aleatório\n",
    "  return countries[index]\n",
    "\n",
    "# Armazena a função choose_country() como UDF \n",
    "choose_country_udf = udf(choose_country, StringType())\n",
    "# Cria uma nova coluna chamada Pais e aplica a função anterior\n",
    "df_nomes = df_nomes.withColumn('Pais', choose_country_udf(\"Nomes\"))\n",
    "\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada AnoNascimento e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória. \n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+---------+-------------+\n",
      "|             Nomes|Escolaridade|     Pais|AnoNascimento|\n",
      "+------------------+------------+---------+-------------+\n",
      "|      David Staten|    Superior| Paraguai|         1989|\n",
      "|    Heather Rivera| Fundamental|Argentina|         1974|\n",
      "|       Lester Wood|       Medio|  Bolívia|         1972|\n",
      "|  William Sandoval| Fundamental| Suriname|         1989|\n",
      "|   Gwendolyn Hines|       Medio|  Bolívia|         1976|\n",
      "|Constance Mcmillan|       Medio|   Brasil|         1980|\n",
      "|        Billy Diaz| Fundamental|  Equador|         1976|\n",
      "|      Marcos Rolfe| Fundamental|  Bolívia|         2007|\n",
      "|         Cara Cole|    Superior|Venezuela|         1985|\n",
      "|         John Gant| Fundamental|  Equador|         1977|\n",
      "+------------------+------------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa as bibliotecas e funções necessárias\n",
    "from random import randint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "# Define a função que gera valores aleatórios entre 1945 e 2010\n",
    "def random_year(x):\n",
    "    # Usa a função SHA-256 hash para gerar um valor de hash de 32-byte\n",
    "    hash_value = hashlib.sha256(str(x).encode()).digest()\n",
    "    # Converte os primeiros 4 bytes do valor do hash em inteiro\n",
    "    random_int = int.from_bytes(hash_value[:4], byteorder=\"big\")\n",
    "    # Calcula o ano leatório entre 1945 e 2010\n",
    "    year = 1945 + (random_int % 65)\n",
    "    # Retorna o ano aleatório\n",
    "    return year\n",
    "# Armazena a função random_year() como um UDF\n",
    "random_year_udf = udf(random_year, IntegerType())\n",
    "# Cria uma nova coluna chamada AnoNascimento e aplica a função anterior\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", random_year_udf(\"Nomes\"))\n",
    "# Exibe 10 linhas do dataframe\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o método select do dataframe (df_nomes), selecione as pessoas que nasceram neste século. Armazene o resultado em outro dataframe chamado df_select e mostre 10 nomes deste."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|          Nomes|AnoNascimento|\n",
      "+---------------+-------------+\n",
      "|   Marcos Rolfe|         2007|\n",
      "|  Lola Gauthier|         2009|\n",
      "|    Kari Dewolf|         2001|\n",
      "|    Jamie Mills|         2004|\n",
      "|   Samuel Polak|         2003|\n",
      "|      Jamie Low|         2002|\n",
      "|  Sallie Gammon|         2001|\n",
      "|Aaron Kondracki|         2007|\n",
      "|   Corinna Hood|         2001|\n",
      "|    Anna Schaus|         2009|\n",
      "+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Seleciona e armazena as colunas Nomes e AnoNascimento quando o ano é maior ou igual a 2000 em um novo dataframe \n",
    "df_select = df_nomes.select('Nomes', 'AnoNascimento').where(\"AnoNascimento > 2000\")\n",
    "# Exibe 10 linhas do novo dataframe\n",
    "df_select.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando Spark SQL repita o processo da Pergunta 6. Lembre-se que, para trabalharmos com SparkSQL, precisamos registrar uma tabela temporária e depois executar o comando SQL. Abaixo um exemplo de como executar comandos SQL com SparkSQL:\n",
    "\n",
    "```df_nomes.registerTempTable(\"pessoas\")\n",
    "\n",
    "sqlContext.sql(\"select * from pessoas\").show()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando Spark SQL repita o processo da Pergunta 6. Lembre-se que, para trabalharmos com SparkSQL, precisamos registrar uma tabela temporária e depois executar o comando SQL. Abaixo um exemplo de como executar comandos SQL com SparkSQL:\n",
    "\n",
    "```\n",
    "df_nomes.registerTempTable(\"pessoas\")\n",
    "\n",
    "sqlContext.sql(\"select * from pessoas\").show()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|          Nomes|AnoNascimento|\n",
      "+---------------+-------------+\n",
      "|   Marcos Rolfe|         2007|\n",
      "|  Lola Gauthier|         2009|\n",
      "|    Kari Dewolf|         2001|\n",
      "|    Jamie Mills|         2004|\n",
      "|   Samuel Polak|         2003|\n",
      "|      Jamie Low|         2002|\n",
      "|  Sallie Gammon|         2001|\n",
      "|Aaron Kondracki|         2007|\n",
      "|   Corinna Hood|         2001|\n",
      "|    Anna Schaus|         2009|\n",
      "|   Samuel Polak|         2003|\n",
      "|Rebecca Marquez|         2003|\n",
      "|  Daniel Pigeon|         2004|\n",
      "|   Marcos Rolfe|         2007|\n",
      "|   Ruby Goodell|         2002|\n",
      "|    James Klein|         2007|\n",
      "|      Mary Camp|         2007|\n",
      "|   Dixie Mercer|         2008|\n",
      "|  Cody Lucchese|         2007|\n",
      "|     Terry Wood|         2005|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Registra a tabela temporária\n",
    "df_nomes.registerTempTable(\"pessoas\")\n",
    "# Executa o comando sql\n",
    "df_select = spark.sql(\n",
    "    \"\"\"select Nomes, AnoNascimento\n",
    "    from pessoas\n",
    "    where AnoNascimento > 2000\"\"\"\n",
    ")\n",
    "# Exibe o resultado\n",
    "df_select.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o método select do Dataframe df_nomes, Conte o número de pessoas que são da geração Millennials (nascidos entre 1980 e 1994) no Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:============================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Millennials: 2269068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Seleciona e armazena a colunas Nomes quando o ano está entre 1980 e 1994\n",
    "df_millennials = df_nomes.select('Nomes').where(\"AnoNascimento between 1980 and 1994\")\n",
    "# Conta a quantidade de linhas e armazena em uma variável\n",
    "count_millennials =  df_millennials.count()\n",
    "# Exibe o resultado\n",
    "print('Millennials:',count_millennials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repita o processo da Pergunta 8 utilizando Spark SQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Millennials|\n",
      "+-----------+\n",
      "|    2269068|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Executa o comando sql\n",
    "df_select = spark.sql(\n",
    "    \"\"\"select count(Nomes) as Millennials\n",
    "    from pessoas\n",
    "    where AnoNascimento between 1980 and 1994\n",
    "    \"\"\"\n",
    ")\n",
    "# Exibe o resultado\n",
    "df_select.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando Spark SQL, obtenha a quantidade de pessoas de cada país para uma das gerações abaixo. Armazene o resultado em um novo dataframe e depois mostre todas as linhas em ordem crescente de Pais, Geração e Quantidade\n",
    "\n",
    "- Baby Boomers – nascidos entre 1944 e 1964;\n",
    "\n",
    "- Geração X – nascidos entre 1965 e 1979;\n",
    "\n",
    "- Millennials (Geração Y) – nascidos entre 1980 e 1994;\n",
    "\n",
    "- Geração Z – nascidos entre 1995 e 2015."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------------+\n",
      "|     Pais|     Geracao|QuantidadePessoas|\n",
      "+---------+------------+-----------------+\n",
      "|Argentina|Baby Boomers|           279995|\n",
      "|Argentina|   Geração X|           203229|\n",
      "|Argentina|   Geração Z|           183468|\n",
      "|Argentina| Millennials|           186933|\n",
      "|  Bolívia|Baby Boomers|           230201|\n",
      "|  Bolívia|   Geração X|           176548|\n",
      "|  Bolívia|   Geração Z|           169703|\n",
      "|  Bolívia| Millennials|           170518|\n",
      "|   Brasil|Baby Boomers|           303896|\n",
      "|   Brasil|   Geração X|           157202|\n",
      "|   Brasil|   Geração Z|           153307|\n",
      "|   Brasil| Millennials|           169124|\n",
      "|    Chile|Baby Boomers|           240553|\n",
      "|    Chile|   Geração X|           183925|\n",
      "|    Chile|   Geração Z|           226742|\n",
      "|    Chile| Millennials|           163349|\n",
      "| Colômbia|Baby Boomers|           259089|\n",
      "| Colômbia|   Geração X|           193126|\n",
      "| Colômbia|   Geração Z|           163303|\n",
      "| Colômbia| Millennials|           189319|\n",
      "+---------+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Executa o comando sql\n",
    "df_count_country_generation = spark.sql(\n",
    "\"\"\"select Pais,\n",
    "       case \n",
    "           when AnoNascimento between 1944 and 1964 then 'Baby Boomers'\n",
    "           when AnoNascimento between 1965 and 1979 then 'Geração X'\n",
    "           when AnoNascimento between 1980 and 1994 then 'Millennials'\n",
    "           when AnoNascimento between 1995 and 2015 then 'Geração Z'\n",
    "       end as Geracao,\n",
    "       count(*) as QuantidadePessoas\n",
    "from pessoas\n",
    "group by Pais, Geracao\n",
    "order by Pais, Geracao, QuantidadePessoas\n",
    "\"\"\"\n",
    ")\n",
    "# Exibe o resultado\n",
    "df_count_country_generation.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
