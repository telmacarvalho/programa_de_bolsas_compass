{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente iremos preparar o ambiente, definindo o diretório onde nosso código será desenvolvido. Para este diretório iremos copiar o arquivo nomes_aleatorios.txt.\n",
    "\n",
    "Após, em nosso script Python, devemos importar as bibliotecas necessárias:\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "```\n",
    "Aplicando as bibliotecas do Spark, podemos definir a Spark Session e sobre ela definir o Context para habilitar o módulo SQL\n",
    "\n",
    "```spark = SparkSession \\\n",
    "\n",
    "                .builder \\\n",
    "\n",
    "                .master(\"local[*]\")\\\n",
    "\n",
    "                .appName(\"Exercicio Intro\") \\\n",
    "\n",
    "                .getOrCreate()```\n",
    "\n",
    "Nesta etapa, adicione código para ler o arquivo nomes_aleatorios.txt através do comando spark.read.csv. Carregue-o para dentro de um dataframe chamado df_nomes e, por fim, liste algumas linhas através do método show. Exemplo: df_nomes.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             _c0|\n",
      "+----------------+\n",
      "|    David Staten|\n",
      "|  Heather Rivera|\n",
      "|     Lester Wood|\n",
      "|William Sandoval|\n",
      "| Gwendolyn Hines|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa a biblioteca findspark para iniciar o serviço do Spark na máquina que hospeda o meu notebook\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "# Importa demais bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "# Aplica as bibliotefcas do spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"Exercicio Intro\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lê o arquivo de texto\n",
    "df_nomes = spark.read.csv(\"Files/nomes_aleatorios.txt\")\n",
    "\n",
    "# Exibe 5 linhas do dataframe\n",
    "df_nomes.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Python, é possível acessar uma coluna de um objeto dataframe pelo atributo (por exemplo df_nomes.nome) ou por índice (df_nomes['nome']). Enquanto a primeira forma é conveniente para a exploração de dados interativos, você deve usar o formato de índice, pois caso algum nome de coluna não esteja de acordo seu código irá falhar.\n",
    "\n",
    "Como não informamos no momento da leitura do arquivo, o Spark não identificou o Schema por padrão e definiu todas as colunas como string. Para ver o Schema, use o método df_nomes.printSchema().\n",
    "\n",
    "Nesta etapa, será necessário adicionar código para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nomes: string (nullable = true)\n",
      "\n",
      "+------------------+\n",
      "|             Nomes|\n",
      "+------------------+\n",
      "|      David Staten|\n",
      "|    Heather Rivera|\n",
      "|       Lester Wood|\n",
      "|  William Sandoval|\n",
      "|   Gwendolyn Hines|\n",
      "|Constance Mcmillan|\n",
      "|        Billy Diaz|\n",
      "|      Marcos Rolfe|\n",
      "|         Cara Cole|\n",
      "|         John Gant|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeia a coluna para Nomes\n",
    "df_nomes = df_nomes.withColumnRenamed(\"_c0\", \"Nomes\")\n",
    "# Exibe o esquema\n",
    "df_nomes.printSchema()\n",
    "# Exibe 10 linhas do dataframe\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada Escolaridade e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Medio ou Superior.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+---------------+\n",
      "|             Nomes|Escolaridade|           pais|\n",
      "+------------------+------------+---------------+\n",
      "|      David Staten|    Superior|       Paraguai|\n",
      "|    Heather Rivera|       Medio|       Suriname|\n",
      "|       Lester Wood| Fundamental|        Uruguai|\n",
      "|  William Sandoval|       Medio|           Peru|\n",
      "|   Gwendolyn Hines|       Medio|Guiana Francesa|\n",
      "|Constance Mcmillan| Fundamental|        Bolívia|\n",
      "|        Billy Diaz|    Superior|          Chile|\n",
      "|      Marcos Rolfe| Fundamental|       Colômbia|\n",
      "|         Cara Cole|    Superior|        Bolívia|\n",
      "|         John Gant|    Superior|        Bolívia|\n",
      "+------------------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa a biblioteca functions\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_nomes = df_nomes.withColumn(\n",
    "  # Gera uma coluna com nome Escolaridade com um array contendo os valores declarados\n",
    "  'Escolaridade',\n",
    "  f.array(\n",
    "  f.lit('Fundamental'),\n",
    "  f.lit('Medio'),\n",
    "  f.lit('Superior'),\n",
    "  # Seleciona de forma aleatória os valores declarados para a coluna Escolaridade\n",
    "  ).getItem((f.rand()*3).cast(\"int\")\n",
    "  )\n",
    ")\n",
    "# Exibe 10 linhas do dataframe\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada Pais e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+---------------+\n",
      "|             Nomes|Escolaridade|           pais|\n",
      "+------------------+------------+---------------+\n",
      "|      David Staten|    Superior|         Brasil|\n",
      "|    Heather Rivera|       Medio|       Suriname|\n",
      "|       Lester Wood| Fundamental|        Bolívia|\n",
      "|  William Sandoval|       Medio|       Suriname|\n",
      "|   Gwendolyn Hines|       Medio|       Paraguai|\n",
      "|Constance Mcmillan| Fundamental|       Colômbia|\n",
      "|        Billy Diaz|    Superior|        Uruguai|\n",
      "|      Marcos Rolfe| Fundamental|         Brasil|\n",
      "|         Cara Cole|    Superior|Guiana Francesa|\n",
      "|         John Gant|    Superior|      Argentina|\n",
      "+------------------+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa as bibliotecas necessárias\n",
    "import random\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "# Gera um array contendo o nome dos paíes da América do Sul\n",
    "countries = ['Argentina', 'Bolívia', 'Brasil', 'Chile', 'Colômbia', 'Equador', 'Guiana', 'Guiana Francesa', 'Paraguai', 'Peru', 'Suriname', 'Uruguai', 'Venezuela']\n",
    "# Gera uma função que seleciona aleatoriamente países da lista anterior\n",
    "def choose():\n",
    "    return random.choice(countries)\n",
    "# Converte a função python collect() em UDF para aplicar no dataframe\n",
    "convert = udf(choose, StringType())\n",
    "# Cria uma nova coluna e aplica a função convertida\n",
    "df_nomes = df_nomes.withColumn(\n",
    "    'pais', convert())\n",
    "# Exibe 10 linhas do dataframe\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada AnoNascimento e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória. \n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
