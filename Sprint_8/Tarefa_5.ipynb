{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente iremos preparar o ambiente, definindo o diretório onde nosso código será desenvolvido. Para este diretório iremos copiar o arquivo nomes_aleatorios.txt.\n",
    "\n",
    "Após, em nosso script Python, devemos importar as bibliotecas necessárias:\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "```\n",
    "Aplicando as bibliotecas do Spark, podemos definir a Spark Session e sobre ela definir o Context para habilitar o módulo SQL\n",
    "\n",
    "```spark = SparkSession \\\n",
    "\n",
    "                .builder \\\n",
    "\n",
    "                .master(\"local[*]\")\\\n",
    "\n",
    "                .appName(\"Exercicio Intro\") \\\n",
    "\n",
    "                .getOrCreate()```\n",
    "\n",
    "Nesta etapa, adicione código para ler o arquivo nomes_aleatorios.txt através do comando spark.read.csv. Carregue-o para dentro de um dataframe chamado df_nomes e, por fim, liste algumas linhas através do método show. Exemplo: df_nomes.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|             _c0|\n",
      "+----------------+\n",
      "|    David Staten|\n",
      "|  Heather Rivera|\n",
      "|     Lester Wood|\n",
      "|William Sandoval|\n",
      "| Gwendolyn Hines|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa a biblioteca findspark para iniciar o serviço do Spark na máquina que hospeda o meu notebook\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "# Importa demais bibliotecas necessárias\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SQLContext\n",
    "\n",
    "# Aplica as bibliotefcas do spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"Exercicio Intro\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lê o arquivo de texto\n",
    "df_nomes = spark.read.csv(\"Files/nomes_aleatorios.txt\")\n",
    "\n",
    "# Exibe 5 linhas do dataframe\n",
    "df_nomes.show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No Python, é possível acessar uma coluna de um objeto dataframe pelo atributo (por exemplo df_nomes.nome) ou por índice (df_nomes['nome']). Enquanto a primeira forma é conveniente para a exploração de dados interativos, você deve usar o formato de índice, pois caso algum nome de coluna não esteja de acordo seu código irá falhar.\n",
    "\n",
    "Como não informamos no momento da leitura do arquivo, o Spark não identificou o Schema por padrão e definiu todas as colunas como string. Para ver o Schema, use o método df_nomes.printSchema().\n",
    "\n",
    "Nesta etapa, será necessário adicionar código para renomear a coluna para Nomes, imprimir o esquema e mostrar 10 linhas do dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nomes: string (nullable = true)\n",
      "\n",
      "+------------------+\n",
      "|             Nomes|\n",
      "+------------------+\n",
      "|      David Staten|\n",
      "|    Heather Rivera|\n",
      "|       Lester Wood|\n",
      "|  William Sandoval|\n",
      "|   Gwendolyn Hines|\n",
      "|Constance Mcmillan|\n",
      "|        Billy Diaz|\n",
      "|      Marcos Rolfe|\n",
      "|         Cara Cole|\n",
      "|         John Gant|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeia a coluna para Nomes\n",
    "df_nomes = df_nomes.withColumnRenamed(\"_c0\", \"Nomes\")\n",
    "# Exibe o esquema\n",
    "df_nomes.printSchema()\n",
    "# Exibe 10 linhas do dataframe\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada Escolaridade e atribua para cada linha um dos três valores de forma aleatória: Fundamental, Medio ou Superior.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|             Nomes|Escolaridade|\n",
      "+------------------+------------+\n",
      "|      David Staten|    Superior|\n",
      "|    Heather Rivera| Fundamental|\n",
      "|       Lester Wood|    Superior|\n",
      "|  William Sandoval| Fundamental|\n",
      "|   Gwendolyn Hines|    Superior|\n",
      "|Constance Mcmillan|       Medio|\n",
      "|        Billy Diaz|       Medio|\n",
      "|      Marcos Rolfe|       Medio|\n",
      "|         Cara Cole|    Superior|\n",
      "|         John Gant|       Medio|\n",
      "+------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa a biblioteca functions\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_nomes = df_nomes.withColumn(\n",
    "  # Gera uma coluna com nome Escolaridade com um array contendo os valores declarados\n",
    "  'Escolaridade',\n",
    "  f.array(\n",
    "  f.lit('Fundamental'),\n",
    "  f.lit('Medio'),\n",
    "  f.lit('Superior'),\n",
    "  # Seleciona de forma aleatória os valores declarados para a coluna Escolaridade\n",
    "  ).getItem((f.rand()*3).cast(\"int\")\n",
    "  )\n",
    ")\n",
    "# Exibe 10 linhas do dataframe\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada Pais e atribua para cada linha o nome de um dos 13 países da América do Sul, de forma aleatória.\n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+---------+\n",
      "|             Nomes|Escolaridade|     Pais|\n",
      "+------------------+------------+---------+\n",
      "|      David Staten|    Superior| Paraguai|\n",
      "|    Heather Rivera| Fundamental|Argentina|\n",
      "|       Lester Wood|    Superior|  Bolívia|\n",
      "|  William Sandoval| Fundamental| Suriname|\n",
      "|   Gwendolyn Hines|    Superior|  Bolívia|\n",
      "|Constance Mcmillan|       Medio|   Brasil|\n",
      "|        Billy Diaz|       Medio|  Equador|\n",
      "|      Marcos Rolfe|       Medio|  Bolívia|\n",
      "|         Cara Cole|    Superior|Venezuela|\n",
      "|         John Gant|       Medio|  Equador|\n",
      "+------------------+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa as bibliotecas e funções necessárias\n",
    "import hashlib\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "# Gera um array contendo o nome dos paíes da América do Sul\n",
    "countries = ['Argentina', 'Bolívia', 'Brasil', 'Chile', 'Colômbia', 'Equador', 'Guiana', 'Guiana Francesa', 'Paraguai', 'Peru', 'Suriname', 'Uruguai', 'Venezuela']\n",
    "\n",
    "# Define a função que seleciona aleatoriamente países da lista anterior\n",
    "def choose_country(x):\n",
    "  # Usa a função SHA-256 hash para gerar um valor de hash de 32-byte\n",
    "  hash_value = hashlib.sha256(str(x).encode()).digest()\n",
    "  # Converte os primeiros 2 bytes do valor do hash em inteiro\n",
    "  random_int = int.from_bytes(hash_value[:2], byteorder=\"big\")\n",
    "  # Calcula o índice do nome aleatório do país aleatório\n",
    "  index = random_int % len(countries)\n",
    "  # Retorna o nome do país aleatório\n",
    "  return countries[index]\n",
    "\n",
    "# Armazena a função choose_country() como UDF \n",
    "choose_country_udf = udf(choose_country, StringType())\n",
    "# Cria uma nova coluna chamada Pais e aplica a função anterior\n",
    "df_nomes = df_nomes.withColumn('Pais', choose_country_udf(\"Nomes\"))\n",
    "\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao dataframe (df_nomes), adicione nova coluna chamada AnoNascimento e atribua para cada linha um valor de ano entre 1945 e 2010, de forma aleatória. \n",
    "\n",
    "Para esta etapa, evite usar funções de iteração, como por exemplo: for, while, entre outras. Dê preferência aos métodos oferecidos para próprio Spark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+---------+-------------+\n",
      "|             Nomes|Escolaridade|     Pais|AnoNascimento|\n",
      "+------------------+------------+---------+-------------+\n",
      "|      David Staten|    Superior| Paraguai|         1989|\n",
      "|    Heather Rivera| Fundamental|Argentina|         1974|\n",
      "|       Lester Wood|    Superior|  Bolívia|         1972|\n",
      "|  William Sandoval| Fundamental| Suriname|         1989|\n",
      "|   Gwendolyn Hines|    Superior|  Bolívia|         1976|\n",
      "|Constance Mcmillan|       Medio|   Brasil|         1980|\n",
      "|        Billy Diaz|       Medio|  Equador|         1976|\n",
      "|      Marcos Rolfe|       Medio|  Bolívia|         2007|\n",
      "|         Cara Cole|    Superior|Venezuela|         1985|\n",
      "|         John Gant|       Medio|  Equador|         1977|\n",
      "+------------------+------------+---------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importa as bibliotecas e funções necessárias\n",
    "from random import randint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "# Define a função que gera valores aleatórios entre 1945 e 2010\n",
    "def random_year(x):\n",
    "    # Usa a função SHA-256 hash para gerar um valor de hash de 32-byte\n",
    "    hash_value = hashlib.sha256(str(x).encode()).digest()\n",
    "    # Converte os primeiros 4 bytes do valor do hash em inteiro\n",
    "    random_int = int.from_bytes(hash_value[:4], byteorder=\"big\")\n",
    "    # Calcula o ano leatório entre 1945 e 2010\n",
    "    year = 1945 + (random_int % 65)\n",
    "    # Retorna o ano aleatório\n",
    "    return year\n",
    "# Armazena a função random_year() como um UDF\n",
    "random_year_udf = udf(random_year, IntegerType())\n",
    "# Cria uma nova coluna chamada AnoNascimento e aplica a função anterior\n",
    "df_nomes = df_nomes.withColumn(\"AnoNascimento\", random_year_udf(\"Nomes\"))\n",
    "# Exibe 10 linhas do dataframe\n",
    "df_nomes.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o método select do dataframe (df_nomes), selecione as pessoas que nasceram neste século. Armazene o resultado em outro dataframe chamado df_select e mostre 10 nomes deste."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|          Nomes|AnoNascimento|\n",
      "+---------------+-------------+\n",
      "|   Marcos Rolfe|         2007|\n",
      "|  Lola Gauthier|         2009|\n",
      "|    Kari Dewolf|         2001|\n",
      "|    Jamie Mills|         2004|\n",
      "|   Samuel Polak|         2003|\n",
      "|      Jamie Low|         2002|\n",
      "|  Sallie Gammon|         2001|\n",
      "|Aaron Kondracki|         2007|\n",
      "|   Corinna Hood|         2001|\n",
      "|    Anna Schaus|         2009|\n",
      "+---------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Seleciona e armazena as colunas Nomes e AnoNascimento quando o ano é maior ou igual a 2000 em um novo dataframe \n",
    "df_select = df_nomes.select('Nomes', 'AnoNascimento').where(\"AnoNascimento > 2000\")\n",
    "# Exibe 10 linhas do novo dataframe\n",
    "df_select.show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando Spark SQL repita o processo da Pergunta 6. Lembre-se que, para trabalharmos com SparkSQL, precisamos registrar uma tabela temporária e depois executar o comando SQL. Abaixo um exemplo de como executar comandos SQL com SparkSQL:\n",
    "\n",
    "```df_nomes.registerTempTable(\"pessoas\")\n",
    "\n",
    "sqlContext.sql(\"select * from pessoas\").show()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
