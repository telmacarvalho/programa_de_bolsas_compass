{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio Parte 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 3: Processamento da Trusted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instruções da tarefa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Processamento -  Camada Trusted**\n",
    "\n",
    "A camada **Trusted** de um data lake corresponde àquela em que os dados encontram-se limpos e são confiáveis. É resultado da integração das diversas fontes de origem, que encontram-se na camada anterior, que chamamos de **Raw**.\n",
    "\n",
    "Aqui faremos uso do Apache Spark no processo, integrando dados existentes na camada Raw Zone. O objetivo é gerar uma visão padronizada dos dados, persistida no S3,  compreendendo a Trusted Zone do data lake.  Nossos jobs Spark serão criados por meio do AWS Glue.\n",
    "\n",
    "Todos os dados serão persistidos na **Trusted** no formato PARQUET, particionados por data de criação do *tweet*  ou data de coleta do TMDB (*dt=<ano\\mês\\dia> exemplo: dt=2018\\03\\31*). A exceção fica para os dados oriundos do processamento *batch (CSV)*, que não precisam ser particionados.\n",
    "\n",
    "Iremos separar o processamento em dois jobs: o primeiro, para carga histórica, será responsável pelo processamento dos arquivos CSV  e o segundo, para carga de dados do Twitter/TMDB. Lembre-se que suas origens serão os dados existentes na RAW Zone.\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "Desenvolva os **jobs** no Glue utilizando a opção **Spark script editor**.  Após, na aba **Job details**, atente para as seguintes opções:\n",
    "\n",
    "- **Worker type**: Informe G 1x (opção de menor configuração).\n",
    "\n",
    "- **Requested  number of workers**: Informe 2, que é a quantidade mínima.\n",
    "\n",
    "- **Job timeout (minutes)**: Mantenha em 60 ou menos, se possível."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "## Define o bucket S3 de origem e o caminho do arquivo para os dados JSON\n",
    "source_bucket = \"data-lake-da-telma\"\n",
    "source_file_path = \"Raw/TMDB/JSON/Superhero/2023/04/13/superhero.json\"\n",
    "collection_date = \"2023/04/13\"\n",
    "\n",
    "\n",
    "## Cria GlueContext e SparkContext\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "\n",
    "## Crea a DynamicFrame dos dados de origem no formato JSON\n",
    "source_dyf = glueContext.create_dynamic_frame_from_options(\n",
    "    \"s3\",\n",
    "    {\"paths\": [f\"s3://{source_bucket}/{source_file_path}\"]},\n",
    "    \"json\"\n",
    ")\n",
    "\n",
    "## Define o bucket S3 de destino e o caminho do arquivo para os dados do Parquet na zona Truested\n",
    "destination_bucket = \"data-lake-da-telma\"\n",
    "destination_file_path = f\"Trusted/{collection_date}.parquet\"\n",
    "\n",
    "## Converte DynamicFrame em um DataFrame\n",
    "source_df = source_dyf.toDF()\n",
    "\n",
    "## Converte o DataFrame em um DynamicFrame no formato Parquet\n",
    "destination_dyf = DynamicFrame.fromDF(\n",
    "    source_df,\n",
    "    glueContext,\n",
    "    \"parquet\"\n",
    ")\n",
    "\n",
    "## Salva o DynamicFrame no formato Parquet no bucket S3 de destino com caminho do arquivo na zona trusted\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame = destination_dyf,\n",
    "    connection_type = \"s3\",\n",
    "    connection_options = {\"path\": f\"s3://{destination_bucket}/{destination_file_path}\"},\n",
    "    format = \"parquet\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"Prints/job_run.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
